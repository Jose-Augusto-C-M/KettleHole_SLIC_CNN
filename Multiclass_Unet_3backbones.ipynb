{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiclass_Unet_3backbones.ipynb",
      "provenance": [],
      "mount_file_id": "1fHh2rq_zyrwA7j8y_RDcwzZl8d61CAfu",
      "authorship_tag": "ABX9TyMeNv/mcOuWPQ797u5hC2ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jose-Augusto-C-M/KettleHole_SLIC_CNN/blob/main/Multiclass_Unet_3backbones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "in6yBngOZGVD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhk1RKTTyI1_",
        "outputId": "bd41ab16-340a-4d4c-b121-e0ddfb71cb9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.5.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/datasets/projeto_marlene/simple_multi_unet_model.py /content"
      ],
      "metadata": {
        "id": "-yjN5z-HZL3Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_multi_unet_model import multi_unet_model\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "HhSCGPqhbdAb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Jmss5ZbVeYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d231bf6-b3f0-4220-aa3e-89caed112f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Models: using `keras` framework.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import normalize\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import segmentation_models as sm\n",
        "\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.metrics import MeanIoU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE_X = 512\n",
        "SIZE_Y = 512\n",
        "n_classes = 17"
      ],
      "metadata": {
        "id": "8GpuMg3kYla3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob('/content/drive/MyDrive/datasets/projeto_marlene/DL_train/ImageOutTif'):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, '*.tif')):\n",
        "        img = cv2.imread(img_path, 1)\n",
        "        train_images.append(img)\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "\n",
        "train_masks = []\n",
        "\n",
        "for directory_path in glob.glob('/content/drive/MyDrive/datasets/projeto_marlene/DL_train/MaskOutTif'):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, '*.tif')):\n",
        "        mask = cv2.imread(mask_path,-1)\n",
        "        train_masks.append(mask)\n",
        "\n",
        "train_masks = np.array(train_masks)"
      ],
      "metadata": {
        "id": "g0JPaooTdBWL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(train_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVuCELVHhqmF",
        "outputId": "14e87d02-816c-453d-eb34-6aad1b8b8265"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(644, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(train_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "080lUzdNh4XU",
        "outputId": "c06e5197-670f-4389-ede9-34fab48f6101"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(644, 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(train_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9hDr-EHZREO",
        "outputId": "2c8608c7-6d47-40bf-d8d4-94d3c40f04b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
              "      dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "np.unique(train_masks_encoded_original_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__iYk8jYg1sw",
        "outputId": "e3abce95-d564-40b2-9ec9-2e8a79d37037"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)"
      ],
      "metadata": {
        "id": "PspoXXex3_cS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_masks_reshaped_encoded = np.expand_dims(train_masks_reshaped_encoded, axis = 1)  \n",
        "np.shape(train_masks_reshaped_encoded)"
      ],
      "metadata": {
        "id": "54Lf6xCLnDlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.unique(train_masks)"
      ],
      "metadata": {
        "id": "cyPVwRtvkoxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_images = np.expand_dims(train_images, axis=3)\n",
        "# train_images = normalize(train_images, axis=1)\n",
        "\n",
        "#Create a subset of data for quick testing\n",
        "#Picking 10% for testing and remaining for training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
        "\n",
        "#Further split training data t a smaller subset for quick testing of models\n",
        "# X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
        "\n",
        "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oc4Z_JahOmc",
        "outputId": "16b033cd-df55-4e38-e04a-0dac92ae7036"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class values in the dataset are ...  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.shape(X_train)"
      ],
      "metadata": {
        "id": "46izt9rU2WsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.unique(train_images)"
      ],
      "metadata": {
        "id": "MEygDLsgZy6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
        "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
        "\n",
        "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
      ],
      "metadata": {
        "id": "ca4wj97JhtKQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(train_masks_cat)"
      ],
      "metadata": {
        "id": "MXejDTf7avNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(y_train_cat)"
      ],
      "metadata": {
        "id": "NeRD44Uab8Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(test_masks_cat)"
      ],
      "metadata": {
        "id": "fEBFgVGfcFMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(y_test_cat)"
      ],
      "metadata": {
        "id": "X3P15Rx1cKBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm.set_framework('tf.keras')\n",
        "\n",
        "sm.framework()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fBZgKu-Ayc1o",
        "outputId": "441a15b0-23de-4df1-f9b9-84f3e7959eff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tf.keras'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes=17\n",
        "activation='softmax'\n",
        "\n",
        "LR = 0.0001\n",
        "optim = tf.keras.optimizers.Adam(LR)\n",
        "\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.058, 0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058,0.058])) \n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]"
      ],
      "metadata": {
        "id": "w319LsEQxfF_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(X_train)"
      ],
      "metadata": {
        "id": "8e1MHLSbzLdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BACKBONE1 = 'resnet34'\n",
        "preprocess_input1 = sm.get_preprocessing(BACKBONE1)\n",
        "\n",
        "# preprocess input\n",
        "X_train1 = preprocess_input1(X_train)\n",
        "X_test1 = preprocess_input1(X_test)\n",
        "\n",
        "# define model\n",
        "model1 = sm.Unet(BACKBONE1, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "#model1.compile(optim, total_loss, metrics=metrics)\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "# print(model1.summary())\n",
        "\n",
        "\n",
        "history1=model1.fit(X_train1, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test1, y_test_cat))\n",
        "\n",
        "\n",
        "model1.save('res34_backbone_50epochs.hdf5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg6jsWYGyU2n",
        "outputId": "a63fc5ac-a47b-4694-c795-db3db972dc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "73/73 [==============================] - 762s 10s/step - loss: 2.1839 - iou_score: 0.5646 - f1-score: 0.5649 - val_loss: 4.3384 - val_iou_score: 0.5855 - val_f1-score: 0.5918\n",
            "Epoch 2/50\n",
            "73/73 [==============================] - 757s 10s/step - loss: 1.9665 - iou_score: 0.5863 - f1-score: 0.5880 - val_loss: 36378.6797 - val_iou_score: 0.5605 - val_f1-score: 0.5698\n",
            "Epoch 3/50\n",
            "26/73 [=========>....................] - ETA: 7:57 - loss: 1.9991 - iou_score: 0.5633 - f1-score: 0.5633"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Model 2\n",
        "\n",
        "BACKBONE2 = 'inceptionv3'\n",
        "preprocess_input2 = sm.get_preprocessing(BACKBONE2)\n",
        "\n",
        "# preprocess input\n",
        "X_train2 = preprocess_input2(X_train)\n",
        "X_test2 = preprocess_input2(X_test)\n",
        "\n",
        "# define model\n",
        "model2 = sm.Unet(BACKBONE2, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model2.compile(optim, total_loss, metrics)\n",
        "#model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "\n",
        "print(model2.summary())\n",
        "\n",
        "\n",
        "history2=model2.fit(X_train2, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test2, y_test_cat))\n",
        "\n",
        "\n",
        "model2.save('inceptionv3_backbone_50epochs.hdf5')"
      ],
      "metadata": {
        "id": "EXZ0VPUm5xXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BACKBONE3 = 'vgg16'\n",
        "preprocess_input3 = sm.get_preprocessing(BACKBONE3)\n",
        "\n",
        "# preprocess input\n",
        "X_train3 = preprocess_input3(X_train)\n",
        "X_test3 = preprocess_input3(X_test)\n",
        "\n",
        "\n",
        "# define model\n",
        "model3 = sm.Unet(BACKBONE3, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model3.compile(optim, total_loss, metrics)\n",
        "#model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "\n",
        "print(model3.summary())\n",
        "\n",
        "history3=model3.fit(X_train3, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test3, y_test_cat))\n",
        "\n",
        "\n",
        "model3.save('vgg19_backbone_50epochs.hdf5')"
      ],
      "metadata": {
        "id": "EcgVQBIm5xdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history1.history['iou_score']\n",
        "val_acc = history1.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
        "plt.title('Training and validation IOU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bos9p9w9597T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced', \n",
        "                                                  classes = np.unique(train_masks_reshaped_encoded), \n",
        "                                                  y = train_masks_reshaped_encoded\n",
        "                                                 )\n",
        "print(\"Class weights are...:\", class_weights)"
      ],
      "metadata": {
        "id": "Dle0fiYDh9PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight( \"balanced\", np.unique(train_masks_reshaped_encoded), train_masks_reshaped_encoded)\n",
        "class_weights = dict(zip(np.unique(train_masks_reshaped_encoded), class_weights))\n",
        "class_weights"
      ],
      "metadata": {
        "id": "8PTYje8LXrSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMG_HEIGHT = X_train.shape[1]\n",
        "IMG_WIDTH  = X_train.shape[2]\n",
        "IMG_CHANNELS = X_train.shape[3]\n",
        "\n",
        "def get_model():\n",
        "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
        "\n",
        "model = get_model()\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()        "
      ],
      "metadata": {
        "id": "yCHBkaCXfbcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.shape(X_train)"
      ],
      "metadata": {
        "id": "KSN4cJj5pb5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.shape(y_train_cat)"
      ],
      "metadata": {
        "id": "wmvbZaNWpfiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "newInput = tf.keras.layers.Input(batch_shape=(0,199,199,3))\n",
        "op = model(newInput)"
      ],
      "metadata": {
        "id": "tGgyPZdZqgjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#If starting with pre-trained weights. \n",
        "#model.load_weights('???.hdf5')\n",
        "\n",
        "history = model.fit(X_train, y_train_cat, \n",
        "                    batch_size = 16, \n",
        "                    verbose=1, \n",
        "                    epochs=50, \n",
        "                    validation_data=(X_test, y_test_cat), \n",
        "                    #class_weight=class_weights,\n",
        "                    shuffle=False)"
      ],
      "metadata": {
        "id": "MJpXfAf9lie2"
      }
    }
  ]
}